{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70223bd",
   "metadata": {},
   "source": [
    "This jupyter notebook tries to predict geomagnetic storms in the next hour based on the last N hours using recurrent neural networks (RNN, more especifically GRU) for binary (storm or not) classification.\n",
    "It is divided in 4 parts:\n",
    "* Loading libraries and data.\n",
    "* Transforming the dataset and doing some feature engineering.\n",
    "* Making and fiting the model.\n",
    "* Checking the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc66fcb",
   "metadata": {},
   "source": [
    "# Forecasting geomagnetic storms using recurrent neural networks (RNN) for classification\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcde7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                             roc_curve, precision_recall_curve, average_precision_score, \n",
    "                             fbeta_score, recall_score)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15a00a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244535 entries, 0 to 244534\n",
      "Data columns (total 19 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   datetime     244535 non-null  object \n",
      " 1   ABS_B        244535 non-null  float64\n",
      " 2   F            244535 non-null  float64\n",
      " 3   BX_GSE       244535 non-null  float64\n",
      " 4   BY_GSE       244535 non-null  float64\n",
      " 5   BZ_GSE       244535 non-null  float64\n",
      " 6   SIGMA-ABS_B  244535 non-null  float64\n",
      " 7   SIGMA-B      244535 non-null  float64\n",
      " 8   SIGMA-Bx     244535 non-null  float64\n",
      " 9   SIGMA-By     244535 non-null  float64\n",
      " 10  SIGMA-Bz     244535 non-null  float64\n",
      " 11  T            244535 non-null  float64\n",
      " 12  N            244535 non-null  float64\n",
      " 13  V            244535 non-null  float64\n",
      " 14  Ratio        244535 non-null  float64\n",
      " 15  Pressure     244535 non-null  float64\n",
      " 16  R            244535 non-null  int64  \n",
      " 17  DST          244535 non-null  float64\n",
      " 18  storm_now    244535 non-null  int64  \n",
      "dtypes: float64(16), int64(2), object(1)\n",
      "memory usage: 35.4+ MB\n",
      "None\n",
      "(244535, 19)\n",
      "              datetime  ABS_B    F  BX_GSE  BY_GSE  BZ_GSE  SIGMA-ABS_B  \\\n",
      "0  1995-01-01 00:00:00    4.0  3.9    -0.6     3.2    -2.1          0.6   \n",
      "1  1995-01-01 01:00:00    3.0  2.9    -0.9     2.4    -1.3          0.3   \n",
      "2  1995-01-01 02:00:00    3.2  1.9     0.5     1.8    -0.4          0.7   \n",
      "3  1995-01-01 03:00:00    4.3  4.0    -3.2     2.5     0.3          0.4   \n",
      "4  1995-01-01 04:00:00    4.8  4.8    -4.3     2.1     0.3          0.1   \n",
      "\n",
      "   SIGMA-B  SIGMA-Bx  SIGMA-By  SIGMA-Bz        T     N      V  Ratio  \\\n",
      "0      1.2       0.5       0.9       0.5  15816.0  16.3  315.0  0.016   \n",
      "1      0.8       0.2       0.3       0.8  15601.0  18.8  315.0  0.013   \n",
      "2      2.6       1.6       0.9       1.8  20703.0  19.3  320.0  0.012   \n",
      "3      1.5       1.2       0.5       0.8  25094.0  16.4  317.0  0.013   \n",
      "4      0.3       0.2       0.2       0.2  25604.0  14.5  313.0  0.015   \n",
      "\n",
      "   Pressure   R  DST  storm_now  \n",
      "0      2.87  13 -2.0          0  \n",
      "1      3.28  13  3.0          0  \n",
      "2      3.46  13  6.0          0  \n",
      "3      2.90  13  5.0          0  \n",
      "4      2.51  13  3.0          0  \n"
     ]
    }
   ],
   "source": [
    "# Load raw dataframe\n",
    "df = pd.read_csv(\"../data/data_storms.csv\")\n",
    "\n",
    "print(df.info())\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f77663",
   "metadata": {},
   "source": [
    "# Changing the dataframe to fit RNN\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9995b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244535 entries, 0 to 244534\n",
      "Data columns (total 19 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   datetime     244535 non-null  datetime64[ns]\n",
      " 1   ABS_B        244535 non-null  float64       \n",
      " 2   F            244535 non-null  float64       \n",
      " 3   BX_GSE       244535 non-null  float64       \n",
      " 4   BY_GSE       244535 non-null  float64       \n",
      " 5   BZ_GSE       244535 non-null  float64       \n",
      " 6   SIGMA-ABS_B  244535 non-null  float64       \n",
      " 7   SIGMA-B      244535 non-null  float64       \n",
      " 8   SIGMA-Bx     244535 non-null  float64       \n",
      " 9   SIGMA-By     244535 non-null  float64       \n",
      " 10  SIGMA-Bz     244535 non-null  float64       \n",
      " 11  T            244535 non-null  float64       \n",
      " 12  N            244535 non-null  float64       \n",
      " 13  V            244535 non-null  float64       \n",
      " 14  Ratio        244535 non-null  float64       \n",
      " 15  Pressure     244535 non-null  float64       \n",
      " 16  R            244535 non-null  int64         \n",
      " 17  DST          244535 non-null  float64       \n",
      " 18  storm_now    244535 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(16), int64(2)\n",
      "memory usage: 35.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'datetime' column is in datetime format\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "020c4fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sum\n",
      "datetime     \n",
      "1995      371\n",
      "1996       31\n",
      "1997      284\n",
      "1998      555\n",
      "1999      424\n",
      "2000      554\n",
      "2001      677\n",
      "2002      904\n",
      "2003      335\n",
      "2004      354\n",
      "2005      533\n",
      "2006       93\n",
      "2007       20\n",
      "2008       12\n",
      "2009       17\n",
      "2010       84\n",
      "2011      201\n",
      "2012      425\n",
      "2013      224\n",
      "2014      163\n",
      "2015      659\n",
      "2016      219\n",
      "2017      213\n",
      "2018       80\n",
      "2019       24\n",
      "2020       21\n",
      "2021       51\n",
      "2022      159\n",
      "2023      386\n",
      "2024      572\n"
     ]
    }
   ],
   "source": [
    "# Analyze storm distribution over time\n",
    "# Number of storm-hours per year\n",
    "df_storms = df.groupby(df['datetime'].dt.year)['storm_now'].agg(['sum'])\n",
    "print(df_storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b756e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hours with storm distribution by year:\n",
      "          sum  count\n",
      "datetime            \n",
      "1995      371   8628\n",
      "1996       31   8578\n",
      "1997      284   8121\n",
      "1998      555   8328\n",
      "1999      424   6840\n",
      "2000      554   7034\n",
      "2001      677   7466\n",
      "2002      904   7613\n",
      "2003      335   6439\n",
      "2004      354   6948\n",
      "2005      533   8508\n",
      "2006       93   7723\n",
      "2007       20   8230\n",
      "2008       12   8649\n",
      "2009       17   8550\n",
      "2010       84   7966\n",
      "2011      201   8313\n",
      "2012      425   8651\n",
      "2013      224   8412\n",
      "2014      163   6897\n",
      "2015      659   8670\n",
      "2016      219   8741\n",
      "2017      213   8712\n",
      "2018       80   8751\n",
      "2019       24   8643\n",
      "2020       21   8722\n",
      "2021       51   8549\n",
      "2022      159   8716\n",
      "2023      386   8614\n",
      "2024      572   8523\n"
     ]
    }
   ],
   "source": [
    "# Number of hours with storms (count should be 8760 [24*365] or 8784 for leap years)\n",
    "# values below 8784 indicate some missing hours\n",
    "yearly_storms = df.groupby(df['datetime'].dt.year)['storm_now'].agg(['sum', 'count'])\n",
    "print(\"\\nHours with storm distribution by year:\")\n",
    "print(yearly_storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81f5298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             datetime  ABS_B    F  BX_GSE  BY_GSE  BZ_GSE  SIGMA-ABS_B  \\\n",
      "0 1995-01-01 00:00:00    4.0  3.9    -0.6     3.2    -2.1          0.6   \n",
      "1 1995-01-01 01:00:00    3.0  2.9    -0.9     2.4    -1.3          0.3   \n",
      "2 1995-01-01 02:00:00    3.2  1.9     0.5     1.8    -0.4          0.7   \n",
      "3 1995-01-01 03:00:00    4.3  4.0    -3.2     2.5     0.3          0.4   \n",
      "4 1995-01-01 04:00:00    4.8  4.8    -4.3     2.1     0.3          0.1   \n",
      "\n",
      "   SIGMA-B  SIGMA-Bx  SIGMA-By  SIGMA-Bz        T     N      V  Ratio  \\\n",
      "0      1.2       0.5       0.9       0.5  15816.0  16.3  315.0  0.016   \n",
      "1      0.8       0.2       0.3       0.8  15601.0  18.8  315.0  0.013   \n",
      "2      2.6       1.6       0.9       1.8  20703.0  19.3  320.0  0.012   \n",
      "3      1.5       1.2       0.5       0.8  25094.0  16.4  317.0  0.013   \n",
      "4      0.3       0.2       0.2       0.2  25604.0  14.5  313.0  0.015   \n",
      "\n",
      "   Pressure   R  DST  storm_now  \n",
      "0      2.87  13 -2.0          0  \n",
      "1      3.28  13  3.0          0  \n",
      "2      3.46  13  6.0          0  \n",
      "3      2.90  13  5.0          0  \n",
      "4      2.51  13  3.0          0  \n"
     ]
    }
   ],
   "source": [
    "# chronological split is important for time series, so let's sort by datetime\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "assert df['datetime'].is_monotonic_increasing\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a80880",
   "metadata": {},
   "source": [
    "# Fitting the model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "985829e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['datetime', 'ABS_B', 'F', 'BX_GSE', 'BY_GSE', 'BZ_GSE', 'SIGMA-ABS_B',\n",
      "       'SIGMA-B', 'SIGMA-Bx', 'SIGMA-By', 'SIGMA-Bz', 'T', 'N', 'V', 'Ratio',\n",
      "       'Pressure', 'R', 'DST', 'storm_now'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# let's check all columns to drop some (in the features) on the next cell\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82f4dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "# I started using the DST feature, and it is very useful, but it is not always available in real-time forecasts\n",
    "# datetime may not be a problem or leak information, but I am excluding it just to be sure\n",
    "feature_cols = [col for col in df.columns if col not in ['datetime', 'DST', 'storm_now']] \n",
    "X = df[feature_cols].values\n",
    "y = df['storm_now'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "effa6d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ABS_B', 'F', 'BX_GSE', 'BY_GSE', 'BZ_GSE', 'SIGMA-ABS_B', 'SIGMA-B',\n",
      "       'SIGMA-Bx', 'SIGMA-By', 'SIGMA-Bz', 'T', 'N', 'V', 'Ratio', 'Pressure',\n",
      "       'R'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Checking feature names\n",
    "print(df[feature_cols].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e587d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b06bc2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244532, 3, 16)\n",
      "(244532,)\n"
     ]
    }
   ],
   "source": [
    "# This function will transform the 2D data (X) into a 3D array with the shape: \n",
    "# [number_of_samples, N_timesteps, number_of_features]\n",
    "def create_sequences(X, y, time_steps):\n",
    "    \"\"\"\n",
    "    Transforms 2D data into 3D sequences for RNN/GRU input.\n",
    "\n",
    "    X: 2D array of features (all time steps)\n",
    "    y: 1D array of targets (all time steps)\n",
    "    time_steps: The lookback window (N)\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    # Loop starts at 'time_steps' to ensure the first sequence is complete.\n",
    "    # The sequence X[i-time_steps:i] (N historical points) predicts y[i] (the next point).\n",
    "    for i in range(time_steps, len(X)):\n",
    "        # Input sequence: data from time t-N up to t-1\n",
    "        Xs.append(X[i-time_steps:i])\n",
    "        # Target: data at time t (what we are predicting)\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# lookback window of N hours\n",
    "time_steps = 3 \n",
    "\n",
    "X_sequences, y_targets = create_sequences(X_scaled, y, time_steps)\n",
    "\n",
    "# Check the new shape\n",
    "print(X_sequences.shape) # Should look like (244532, N, 17)\n",
    "print(y_targets.shape)   # Should look like (244532,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "978fe000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training samples: 195625\n",
      "Test samples: 48907\n",
      "\n",
      "Class distribution in training set:\n",
      "Percentage of positive cases in train: 3.80%\n",
      "\n",
      "Class distribution in test set:\n",
      "Percentage of positive cases in test: 2.48%\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets\n",
    "\n",
    "# Use last 20% as test set and NO RANDOM SPLIT FOR TIME SERIES\n",
    "split_idx = int(len(X_sequences) * 0.8)\n",
    "X_train, X_test = X_sequences[:split_idx], X_sequences[split_idx:]\n",
    "y_train, y_test = y_targets[:split_idx], y_targets[split_idx:]\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(f\"Percentage of positive cases in train: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"\\nClass distribution in test set:\")\n",
    "print(f\"Percentage of positive cases in test: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5515babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final feature count and time steps for the model input layer\n",
    "n_features = X_train.shape[2]\n",
    "time_steps = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e751770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved model found, proceeding to train a new model.\n"
     ]
    }
   ],
   "source": [
    "# you can just load the model if already trained and not run the random search and best model again\n",
    "try:\n",
    "    model = load_model('./Data/storms_keras_RNN.keras')\n",
    "except:\n",
    "    print(\"No saved model found, proceeding to train a new model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1de1ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom metric\n",
    "def f2_score(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    # All subsequent operations will now have matching data types\n",
    "    y_pred = tf.round(y_pred)\n",
    "\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
    "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, tf.float32))\n",
    "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), tf.float32))\n",
    "\n",
    "    beta_sq = 4.0  # 2^2\n",
    "    num = (1.0 + beta_sq) * tp\n",
    "    den = (1.0 + beta_sq) * tp + beta_sq * fn + fp + 1e-8\n",
    "\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc4d1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_features, time_steps, units_gru, units_dense, dropout_rate, learning_rate):\n",
    "\n",
    "    \"\"\"Create model with configurable hyperparameters\"\"\"\n",
    "    model = Sequential([\n",
    "            # 0. Input Layer\n",
    "            Input(shape=(time_steps, n_features)),\n",
    "\n",
    "            # 1. Sequential Layer (GRU)\n",
    "            GRU(units=units_gru, activation='tanh', return_sequences=False),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            # 2. Classification Layer (Dense)\n",
    "            Dense(units=units_dense, activation='relu'),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            # 3. Output Layer\n",
    "            Dense(units=1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate), \n",
    "        loss='binary_crossentropy',\n",
    "        #metrics=[AUC(name='AUC'), Precision(name='Precision'), Recall(name='Recall')]\n",
    "        metrics=[f2_score, 'Recall', 'AUC']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f99f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_search(X_train, y_train, n_iter=20):\n",
    "    \"\"\"Random search over hyperparameters\"\"\"\n",
    "    \n",
    "    # Define hyperparameter ranges\n",
    "    param_distributions = {\n",
    "        'units_gru': [32, 64, 128, 256],\n",
    "        'units_dense': [16, 32, 64],\n",
    "        'dropout_rate': [0.1, 0.3, 0.5],\n",
    "        'learning_rate': [1e-3, 5e-4, 1e-4],\n",
    "        #'time_steps': [3, 6, 12, 24],\n",
    "        'time_steps': [3],\n",
    "        'batch_size': [32, 64, 128]\n",
    "    }\n",
    "    \n",
    "    # support both 2D and 3D inputs\n",
    "    if X_train.ndim == 3:\n",
    "        n_samples, time_steps_from_X, n_features = X_train.shape\n",
    "    else:\n",
    "        n_samples, n_features = X_train.shape\n",
    "        time_steps_from_X = None\n",
    "    \n",
    "    # Compute class weights for imbalanced dataset\n",
    "    class_weights = dict(enumerate(compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )))\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Testing {n_iter} random combinations...\")\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Random sample from each parameter distribution\n",
    "        params = {\n",
    "            'units_gru': int(np.random.choice(param_distributions['units_gru'])),\n",
    "            'units_dense': int(np.random.choice(param_distributions['units_dense'])),\n",
    "            'dropout_rate': float(np.random.choice(param_distributions['dropout_rate'])),\n",
    "            'learning_rate': float(np.random.choice(param_distributions['learning_rate'])),\n",
    "            'time_steps': int(np.random.choice(param_distributions['time_steps'])),\n",
    "            'batch_size': int(np.random.choice(param_distributions['batch_size']))\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n[{i+1}/{n_iter}] Testing: {params}\")\n",
    "        \n",
    "        # If X_train already has time_steps dimension, ensure consistency\n",
    "        if time_steps_from_X is not None and params['time_steps'] != time_steps_from_X:\n",
    "            # skip incompatible combinations\n",
    "            print(f\"  -> Skipping params because X_train.time_steps={time_steps_from_X} != sampled time_steps={params['time_steps']}\")\n",
    "            continue\n",
    "        \n",
    "        # Create model (exclude batch_size from model creation)\n",
    "        model_params = {k: v for k, v in params.items() if k != 'batch_size'}\n",
    "        model = create_model(n_features=n_features, **model_params)\n",
    "        \n",
    "        es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=50,\n",
    "            batch_size=params['batch_size'],\n",
    "            callbacks=[es],\n",
    "            verbose=0,\n",
    "            class_weight=class_weights\n",
    "        )\n",
    "        \n",
    "        # Get best validation (defensive access)\n",
    "        #val_recall = max(history.history.get('val_recall', [0]))\n",
    "        val_f2 = max(history.history.get('val_f2_score', [0]))\n",
    "        print(f\"  -> Val F2: {val_f2:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({**params, 'val_f2': val_f2})\n",
    "        \n",
    "        # Update best\n",
    "        if val_f2 > best_score:\n",
    "            best_score = val_f2\n",
    "            best_params = params\n",
    "    \n",
    "    print(\"Best Parameters:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"  Best Validation F2: {best_score:.4f}\")\n",
    "    \n",
    "    return best_params, best_score, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd54b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 50 random combinations...\n",
      "\n",
      "[1/50] Testing: {'units_gru': 32, 'units_dense': 16, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 64}\n",
      "  -> Val F2: 0.0506\n",
      "\n",
      "[2/50] Testing: {'units_gru': 256, 'units_dense': 64, 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'time_steps': 3, 'batch_size': 128}\n",
      "  -> Val F2: 0.0542\n",
      "\n",
      "[3/50] Testing: {'units_gru': 128, 'units_dense': 32, 'dropout_rate': 0.3, 'learning_rate': 0.0001, 'time_steps': 3, 'batch_size': 64}\n",
      "  -> Val F2: 0.0477\n",
      "\n",
      "[4/50] Testing: {'units_gru': 256, 'units_dense': 64, 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0438\n",
      "\n",
      "[5/50] Testing: {'units_gru': 64, 'units_dense': 16, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0454\n",
      "\n",
      "[6/50] Testing: {'units_gru': 256, 'units_dense': 16, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 64}\n",
      "  -> Val F2: 0.0480\n",
      "\n",
      "[7/50] Testing: {'units_gru': 64, 'units_dense': 16, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 128}\n",
      "  -> Val F2: 0.0529\n",
      "\n",
      "[8/50] Testing: {'units_gru': 32, 'units_dense': 16, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 64}\n",
      "  -> Val F2: 0.0489\n",
      "\n",
      "[9/50] Testing: {'units_gru': 64, 'units_dense': 32, 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'time_steps': 3, 'batch_size': 128}\n",
      "  -> Val F2: 0.0562\n",
      "\n",
      "[10/50] Testing: {'units_gru': 64, 'units_dense': 32, 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0453\n",
      "\n",
      "[11/50] Testing: {'units_gru': 32, 'units_dense': 16, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0439\n",
      "\n",
      "[12/50] Testing: {'units_gru': 32, 'units_dense': 16, 'dropout_rate': 0.5, 'learning_rate': 0.0005, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0488\n",
      "\n",
      "[13/50] Testing: {'units_gru': 32, 'units_dense': 16, 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0458\n",
      "\n",
      "[14/50] Testing: {'units_gru': 128, 'units_dense': 64, 'dropout_rate': 0.3, 'learning_rate': 0.0005, 'time_steps': 3, 'batch_size': 128}\n",
      "  -> Val F2: 0.0541\n",
      "\n",
      "[15/50] Testing: {'units_gru': 32, 'units_dense': 32, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0451\n",
      "\n",
      "[16/50] Testing: {'units_gru': 32, 'units_dense': 16, 'dropout_rate': 0.5, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0482\n",
      "\n",
      "[17/50] Testing: {'units_gru': 32, 'units_dense': 32, 'dropout_rate': 0.5, 'learning_rate': 0.0001, 'time_steps': 3, 'batch_size': 128}\n",
      "  -> Val F2: 0.0699\n",
      "\n",
      "[18/50] Testing: {'units_gru': 256, 'units_dense': 64, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0475\n",
      "\n",
      "[19/50] Testing: {'units_gru': 128, 'units_dense': 64, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0475\n",
      "\n",
      "[20/50] Testing: {'units_gru': 256, 'units_dense': 16, 'dropout_rate': 0.3, 'learning_rate': 0.0005, 'time_steps': 3, 'batch_size': 128}\n",
      "  -> Val F2: 0.0534\n",
      "\n",
      "[21/50] Testing: {'units_gru': 32, 'units_dense': 64, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0453\n",
      "\n",
      "[22/50] Testing: {'units_gru': 64, 'units_dense': 32, 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'time_steps': 3, 'batch_size': 64}\n",
      "  -> Val F2: 0.0479\n",
      "\n",
      "[23/50] Testing: {'units_gru': 32, 'units_dense': 16, 'dropout_rate': 0.3, 'learning_rate': 0.0005, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0458\n",
      "\n",
      "[24/50] Testing: {'units_gru': 128, 'units_dense': 16, 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0437\n",
      "\n",
      "[25/50] Testing: {'units_gru': 256, 'units_dense': 64, 'dropout_rate': 0.3, 'learning_rate': 0.0001, 'time_steps': 3, 'batch_size': 128}\n",
      "  -> Val F2: 0.0536\n",
      "\n",
      "[26/50] Testing: {'units_gru': 32, 'units_dense': 16, 'dropout_rate': 0.3, 'learning_rate': 0.0005, 'time_steps': 3, 'batch_size': 128}\n",
      "  -> Val F2: 0.0561\n",
      "\n",
      "[27/50] Testing: {'units_gru': 128, 'units_dense': 64, 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'time_steps': 3, 'batch_size': 128}\n",
      "  -> Val F2: 0.0528\n",
      "\n",
      "[28/50] Testing: {'units_gru': 256, 'units_dense': 32, 'dropout_rate': 0.5, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 64}\n",
      "  -> Val F2: 0.0501\n",
      "\n",
      "[29/50] Testing: {'units_gru': 64, 'units_dense': 64, 'dropout_rate': 0.5, 'learning_rate': 0.0005, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0452\n",
      "\n",
      "[30/50] Testing: {'units_gru': 128, 'units_dense': 32, 'dropout_rate': 0.1, 'learning_rate': 0.0005, 'time_steps': 3, 'batch_size': 64}\n",
      "  -> Val F2: 0.0490\n",
      "\n",
      "[31/50] Testing: {'units_gru': 64, 'units_dense': 32, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 128}\n",
      "  -> Val F2: 0.0540\n",
      "\n",
      "[32/50] Testing: {'units_gru': 32, 'units_dense': 64, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 128}\n",
      "  -> Val F2: 0.0526\n",
      "\n",
      "[33/50] Testing: {'units_gru': 32, 'units_dense': 16, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 128}\n",
      "  -> Val F2: 0.0553\n",
      "\n",
      "[34/50] Testing: {'units_gru': 128, 'units_dense': 16, 'dropout_rate': 0.5, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 128}\n",
      "  -> Val F2: 0.0595\n",
      "\n",
      "[35/50] Testing: {'units_gru': 256, 'units_dense': 16, 'dropout_rate': 0.5, 'learning_rate': 0.0001, 'time_steps': 3, 'batch_size': 128}\n",
      "  -> Val F2: 0.0570\n",
      "\n",
      "[36/50] Testing: {'units_gru': 32, 'units_dense': 32, 'dropout_rate': 0.5, 'learning_rate': 0.001, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0518\n",
      "\n",
      "[37/50] Testing: {'units_gru': 32, 'units_dense': 16, 'dropout_rate': 0.5, 'learning_rate': 0.0005, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0500\n",
      "\n",
      "[38/50] Testing: {'units_gru': 64, 'units_dense': 64, 'dropout_rate': 0.3, 'learning_rate': 0.0005, 'time_steps': 3, 'batch_size': 32}\n",
      "  -> Val F2: 0.0455\n",
      "\n",
      "[39/50] Testing: {'units_gru': 64, 'units_dense': 32, 'dropout_rate': 0.1, 'learning_rate': 0.0005, 'time_steps': 3, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "# Search for the best hyperparameters\n",
    "best_params, best_score, results = randomized_search(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    n_iter=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620637ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final model with best parameters\n",
    "model = Sequential([\n",
    "    Input(shape=(time_steps, n_features)),\n",
    "    GRU(best_params['units_gru'], activation='tanh'),\n",
    "    Dropout(best_params['dropout_rate']),\n",
    "    Dense(best_params['units_dense'], activation='relu'),\n",
    "    Dropout(best_params['dropout_rate']),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with best learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=best_params['learning_rate']),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[f2_score, 'Recall', 'AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c0818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights for imbalanced dataset\n",
    "class_weights = dict(enumerate(compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70146f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    callbacks=[es],\n",
    "    verbose=1,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(dict(zip(model.metrics_names, results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model.save(\"../data/storms_keras_RNN.keras\")\n",
    "print(\"Best model saved to ../data/storms_keras_RNN.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c6c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred_proba = model.predict(X_test).ravel()     # flatten probabilities\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)       # threshold at 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5632a",
   "metadata": {},
   "source": [
    "# Results\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9049289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "if len(np.unique(y_test)) > 1:\n",
    "    f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
    "    f1_score = fbeta_score(y_test, y_pred, beta=1)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    metrics_table = pd.DataFrame({\n",
    "        'Metric': ['F2', 'F1', 'Recall', 'Avg Precision', 'ROC AUC'],\n",
    "        'Score': [f2_score, f1_score, recall, avg_precision, roc_auc]\n",
    "    })\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(metrics_table.to_string(index=False, float_format=\"%.4f\"))\n",
    "else:\n",
    "    print(\"Only one class present in y_test. Metrics skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze different classification thresholds\n",
    "print(\"\\nThreshold Analysis:\")\n",
    "\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba >= thresh).astype(int)\n",
    "    cm_thresh = confusion_matrix(y_test, y_pred_thresh)\n",
    "    tn, fp, fn, tp = cm_thresh.ravel() if cm_thresh.size == 4 else (cm_thresh[0,0], 0, 0, 0)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nThreshold = {thresh}\")\n",
    "    print(f\"  TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "    print(f\"  Precision: {precision:.3f}, Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "plt.suptitle('Geomagnetic Storm Forecast Using Recurrent Neural Networks', fontsize=16, fontweight='bold')\n",
    "\n",
    "class_names = ['No Storm', 'Storm']\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1, xticklabels=class_names, yticklabels=class_names)\n",
    "ax1.set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('True Label')\n",
    "ax1.set_xlabel('Predicted Label')\n",
    "\n",
    "# 2. Normalized Confusion Matrix\n",
    "cm_norm = pd.DataFrame(cm).apply(lambda x: x/sum(x), axis = 1)\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap='Blues', ax=ax2, xticklabels=class_names, yticklabels=class_names)\n",
    "ax2.set_title('Normalized Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('True Label')\n",
    "ax2.set_xlabel('Predicted Label')\n",
    "\n",
    "# 3. ROC Curve\n",
    "if len(np.unique(y_test)) > 1:\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    ax3.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "    ax3.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "    ax3.set_xlabel('False Positive Rate')\n",
    "    ax3.set_ylabel('True Positive Rate')\n",
    "    ax3.set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Precision-Recall Curve\n",
    "if len(np.unique(y_test)) > 1:\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    ax4.plot(recall, precision, linewidth=2, label=f'PR (AP = {avg_precision:.3f})')\n",
    "    ax4.axhline(y=y_test.mean(), color='k', linestyle='--', linewidth=1, \n",
    "                label=f'Baseline ({y_test.mean():.3f})')\n",
    "    ax4.set_xlabel('Recall')\n",
    "    ax4.set_ylabel('Precision')\n",
    "    ax4.set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Training and Validation Loss\n",
    "if 'history' in locals():\n",
    "    hist = history.history\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    ax5.plot(hist['loss'], label='Train Loss', linewidth=2)\n",
    "    if 'val_loss' in hist:\n",
    "        ax5.plot(hist['val_loss'], label='Val Loss', linewidth=2)\n",
    "    ax5.set_xlabel('Epoch')\n",
    "    ax5.set_ylabel('Loss')\n",
    "    ax5.set_title('Training and Validation Loss', fontsize=12, fontweight='bold')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Prediction Probability Distribution\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "ax6.hist(y_pred_proba[y_test == 0], bins=50, alpha=0.5, label='No Storm (True)', color='blue')\n",
    "ax6.hist(y_pred_proba[y_test == 1], bins=50, alpha=0.5, label='Storm (True)', color='red')\n",
    "ax6.set_xlabel('Predicted Probability')\n",
    "ax6.set_ylabel('Frequency')\n",
    "ax6.set_yscale('log')\n",
    "ax6.set_title('Distribution of Predicted Probabilities', fontsize=12, fontweight='bold')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/geomag_storm_RNN_results.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nVisualization saved as 'geomag_storm_RNN_results.png'\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
