{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70223bd",
   "metadata": {},
   "source": [
    "This jupyter notebook tries to predict geomagnetic storms in the next hour based on the last N hours using recurrent neural networks (RNN, more especifically GRU) for regression (Dst value) classification.\n",
    "It is divided in 4 parts:\n",
    "* Loading libraries and data.\n",
    "* Transforming the dataset and doing some feature engineering.\n",
    "* Making and fiting the model.\n",
    "* Checking the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc66fcb",
   "metadata": {},
   "source": [
    "# Forecasting geomagnetic storms using recurrent neural networks (RNN) for regression\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fcde7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                             roc_curve, precision_recall_curve, average_precision_score, \n",
    "                             fbeta_score, recall_score, mean_absolute_error, mean_squared_error)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from keras import Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import AUC, Precision, Recall\n",
    "\n",
    "from keras.losses import MeanSquaredError, MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "15a00a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244535 entries, 0 to 244534\n",
      "Data columns (total 19 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   datetime     244535 non-null  object \n",
      " 1   ABS_B        244535 non-null  float64\n",
      " 2   F            244535 non-null  float64\n",
      " 3   BX_GSE       244535 non-null  float64\n",
      " 4   BY_GSE       244535 non-null  float64\n",
      " 5   BZ_GSE       244535 non-null  float64\n",
      " 6   SIGMA-ABS_B  244535 non-null  float64\n",
      " 7   SIGMA-B      244535 non-null  float64\n",
      " 8   SIGMA-Bx     244535 non-null  float64\n",
      " 9   SIGMA-By     244535 non-null  float64\n",
      " 10  SIGMA-Bz     244535 non-null  float64\n",
      " 11  T            244535 non-null  float64\n",
      " 12  N            244535 non-null  float64\n",
      " 13  V            244535 non-null  float64\n",
      " 14  Ratio        244535 non-null  float64\n",
      " 15  Pressure     244535 non-null  float64\n",
      " 16  R            244535 non-null  int64  \n",
      " 17  DST          244535 non-null  float64\n",
      " 18  storm_now    244535 non-null  int64  \n",
      "dtypes: float64(16), int64(2), object(1)\n",
      "memory usage: 35.4+ MB\n",
      "None\n",
      "(244535, 19)\n",
      "              datetime  ABS_B    F  BX_GSE  BY_GSE  BZ_GSE  SIGMA-ABS_B  \\\n",
      "0  1995-01-01 00:00:00    4.0  3.9    -0.6     3.2    -2.1          0.6   \n",
      "1  1995-01-01 01:00:00    3.0  2.9    -0.9     2.4    -1.3          0.3   \n",
      "2  1995-01-01 02:00:00    3.2  1.9     0.5     1.8    -0.4          0.7   \n",
      "3  1995-01-01 03:00:00    4.3  4.0    -3.2     2.5     0.3          0.4   \n",
      "4  1995-01-01 04:00:00    4.8  4.8    -4.3     2.1     0.3          0.1   \n",
      "\n",
      "   SIGMA-B  SIGMA-Bx  SIGMA-By  SIGMA-Bz        T     N      V  Ratio  \\\n",
      "0      1.2       0.5       0.9       0.5  15816.0  16.3  315.0  0.016   \n",
      "1      0.8       0.2       0.3       0.8  15601.0  18.8  315.0  0.013   \n",
      "2      2.6       1.6       0.9       1.8  20703.0  19.3  320.0  0.012   \n",
      "3      1.5       1.2       0.5       0.8  25094.0  16.4  317.0  0.013   \n",
      "4      0.3       0.2       0.2       0.2  25604.0  14.5  313.0  0.015   \n",
      "\n",
      "   Pressure   R  DST  storm_now  \n",
      "0      2.87  13 -2.0          0  \n",
      "1      3.28  13  3.0          0  \n",
      "2      3.46  13  6.0          0  \n",
      "3      2.90  13  5.0          0  \n",
      "4      2.51  13  3.0          0  \n"
     ]
    }
   ],
   "source": [
    "# Load raw dataframe\n",
    "df = pd.read_csv(\"../data/data_storms.csv\")\n",
    "\n",
    "print(df.info())\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f77663",
   "metadata": {},
   "source": [
    "# Changing the dataframe to fit RNN\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9995b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244535 entries, 0 to 244534\n",
      "Data columns (total 19 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   datetime     244535 non-null  datetime64[ns]\n",
      " 1   ABS_B        244535 non-null  float64       \n",
      " 2   F            244535 non-null  float64       \n",
      " 3   BX_GSE       244535 non-null  float64       \n",
      " 4   BY_GSE       244535 non-null  float64       \n",
      " 5   BZ_GSE       244535 non-null  float64       \n",
      " 6   SIGMA-ABS_B  244535 non-null  float64       \n",
      " 7   SIGMA-B      244535 non-null  float64       \n",
      " 8   SIGMA-Bx     244535 non-null  float64       \n",
      " 9   SIGMA-By     244535 non-null  float64       \n",
      " 10  SIGMA-Bz     244535 non-null  float64       \n",
      " 11  T            244535 non-null  float64       \n",
      " 12  N            244535 non-null  float64       \n",
      " 13  V            244535 non-null  float64       \n",
      " 14  Ratio        244535 non-null  float64       \n",
      " 15  Pressure     244535 non-null  float64       \n",
      " 16  R            244535 non-null  int64         \n",
      " 17  DST          244535 non-null  float64       \n",
      " 18  storm_now    244535 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(16), int64(2)\n",
      "memory usage: 35.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'datetime' column is in datetime format\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "020c4fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sum\n",
      "datetime     \n",
      "1995      371\n",
      "1996       31\n",
      "1997      284\n",
      "1998      555\n",
      "1999      424\n",
      "2000      554\n",
      "2001      677\n",
      "2002      904\n",
      "2003      335\n",
      "2004      354\n",
      "2005      533\n",
      "2006       93\n",
      "2007       20\n",
      "2008       12\n",
      "2009       17\n",
      "2010       84\n",
      "2011      201\n",
      "2012      425\n",
      "2013      224\n",
      "2014      163\n",
      "2015      659\n",
      "2016      219\n",
      "2017      213\n",
      "2018       80\n",
      "2019       24\n",
      "2020       21\n",
      "2021       51\n",
      "2022      159\n",
      "2023      386\n",
      "2024      572\n"
     ]
    }
   ],
   "source": [
    "# Analyze storm distribution over time\n",
    "# Number of storm-hours per year\n",
    "df_storms = df.groupby(df['datetime'].dt.year)['storm_now'].agg(['sum'])\n",
    "print(df_storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b756e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hours with storm distribution by year:\n",
      "          sum  count\n",
      "datetime            \n",
      "1995      371   8628\n",
      "1996       31   8578\n",
      "1997      284   8121\n",
      "1998      555   8328\n",
      "1999      424   6840\n",
      "2000      554   7034\n",
      "2001      677   7466\n",
      "2002      904   7613\n",
      "2003      335   6439\n",
      "2004      354   6948\n",
      "2005      533   8508\n",
      "2006       93   7723\n",
      "2007       20   8230\n",
      "2008       12   8649\n",
      "2009       17   8550\n",
      "2010       84   7966\n",
      "2011      201   8313\n",
      "2012      425   8651\n",
      "2013      224   8412\n",
      "2014      163   6897\n",
      "2015      659   8670\n",
      "2016      219   8741\n",
      "2017      213   8712\n",
      "2018       80   8751\n",
      "2019       24   8643\n",
      "2020       21   8722\n",
      "2021       51   8549\n",
      "2022      159   8716\n",
      "2023      386   8614\n",
      "2024      572   8523\n"
     ]
    }
   ],
   "source": [
    "# Number of hours with storms (count should be 8760 [24*365] or 8784 for leap years)\n",
    "# values below 8784 indicate some missing data\n",
    "yearly_storms = df.groupby(df['datetime'].dt.year)['storm_now'].agg(['sum', 'count'])\n",
    "print(\"\\nHours with storm distribution by year:\")\n",
    "print(yearly_storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81f5298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             datetime  ABS_B    F  BX_GSE  BY_GSE  BZ_GSE  SIGMA-ABS_B  \\\n",
      "0 1995-01-01 00:00:00    4.0  3.9    -0.6     3.2    -2.1          0.6   \n",
      "1 1995-01-01 01:00:00    3.0  2.9    -0.9     2.4    -1.3          0.3   \n",
      "2 1995-01-01 02:00:00    3.2  1.9     0.5     1.8    -0.4          0.7   \n",
      "3 1995-01-01 03:00:00    4.3  4.0    -3.2     2.5     0.3          0.4   \n",
      "4 1995-01-01 04:00:00    4.8  4.8    -4.3     2.1     0.3          0.1   \n",
      "\n",
      "   SIGMA-B  SIGMA-Bx  SIGMA-By  SIGMA-Bz        T     N      V  Ratio  \\\n",
      "0      1.2       0.5       0.9       0.5  15816.0  16.3  315.0  0.016   \n",
      "1      0.8       0.2       0.3       0.8  15601.0  18.8  315.0  0.013   \n",
      "2      2.6       1.6       0.9       1.8  20703.0  19.3  320.0  0.012   \n",
      "3      1.5       1.2       0.5       0.8  25094.0  16.4  317.0  0.013   \n",
      "4      0.3       0.2       0.2       0.2  25604.0  14.5  313.0  0.015   \n",
      "\n",
      "   Pressure   R  DST  storm_now  \n",
      "0      2.87  13 -2.0          0  \n",
      "1      3.28  13  3.0          0  \n",
      "2      3.46  13  6.0          0  \n",
      "3      2.90  13  5.0          0  \n",
      "4      2.51  13  3.0          0  \n"
     ]
    }
   ],
   "source": [
    "# chronological split is important for time series, so let's sort by datetime\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "assert df['datetime'].is_monotonic_increasing\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a80880",
   "metadata": {},
   "source": [
    "# Fitting the model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "985829e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['datetime', 'ABS_B', 'F', 'BX_GSE', 'BY_GSE', 'BZ_GSE', 'SIGMA-ABS_B',\n",
      "       'SIGMA-B', 'SIGMA-Bx', 'SIGMA-By', 'SIGMA-Bz', 'T', 'N', 'V', 'Ratio',\n",
      "       'Pressure', 'R', 'DST', 'storm_now'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# let's check all columns to drop some (in the features) on the next cell\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82f4dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "# I started using the DST feature, and it is very useful, but it is not always available in real-time forecasts\n",
    "# datetime may not be a problem or leak information, but I am excluding it just to be sure\n",
    "feature_cols = [col for col in df.columns if col not in ['datetime', 'DST', 'storm_now']] \n",
    "X = df[feature_cols].values\n",
    "y = df['DST'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "effa6d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ABS_B', 'F', 'BX_GSE', 'BY_GSE', 'BZ_GSE', 'SIGMA-ABS_B', 'SIGMA-B',\n",
      "       'SIGMA-Bx', 'SIGMA-By', 'SIGMA-Bz', 'T', 'N', 'V', 'Ratio', 'Pressure',\n",
      "       'R'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Checking feature names\n",
    "print(df[feature_cols].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e587d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X_scaler = StandardScaler()\n",
    "X_scaled = X_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_scaled = y_scaler.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b06bc2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244532, 3, 16)\n",
      "(244532,)\n"
     ]
    }
   ],
   "source": [
    "# This function will transform the 2D data (X) into a 3D array with the shape: \n",
    "# [number_of_samples, N_timesteps, number_of_features]\n",
    "def create_sequences(X, y, time_steps):\n",
    "    \"\"\"\n",
    "    Transforms 2D data into 3D sequences for RNN/GRU input.\n",
    "\n",
    "    X: 2D array of features (all time steps)\n",
    "    y: 1D array of targets (all time steps)\n",
    "    time_steps: The lookback window (N)\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    # Loop starts at 'time_steps' to ensure the first sequence is complete.\n",
    "    # The sequence X[i-time_steps:i] (N historical points) predicts y[i] (the next point).\n",
    "    for i in range(time_steps, len(X)):\n",
    "        # Input sequence: data from time t-N up to t-1\n",
    "        Xs.append(X[i-time_steps:i])\n",
    "        # Target: data at time t (what we are predicting)\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# lookback window of N hours\n",
    "time_steps = 3 \n",
    "\n",
    "X_sequences, y_targets = create_sequences(X_scaled, y_scaled.ravel(), time_steps)\n",
    "\n",
    "# Check the new shape\n",
    "print(X_sequences.shape) # Should look like (244532, N, 17)\n",
    "print(y_targets.shape)   # Should look like (244532,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "978fe000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training samples: 195625\n",
      "Test samples: 48907\n",
      "\n",
      "Class distribution in training set:\n",
      "Percentage of positive cases in train: -3.71%\n",
      "\n",
      "Class distribution in test set:\n",
      "Percentage of positive cases in test: 14.84%\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets\n",
    "\n",
    "# Use last 20% as test set and NO RANDOM SPLIT FOR TIME SERIES\n",
    "split_idx = int(len(X_sequences) * 0.8)\n",
    "X_train, X_test = X_sequences[:split_idx], X_sequences[split_idx:]\n",
    "y_train, y_test = y_targets[:split_idx], y_targets[split_idx:]\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(f\"Percentage of positive cases in train: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"\\nClass distribution in test set:\")\n",
    "print(f\"Percentage of positive cases in test: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5515babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final feature count and time steps for the model input layer\n",
    "n_features = X_train.shape[2]\n",
    "time_steps = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e751770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved model found, proceeding to train a new model.\n"
     ]
    }
   ],
   "source": [
    "# you can just load the model if already trained and not run the grid search and best model again\n",
    "try:\n",
    "    model = load_model('./Data/storms_keras_RNN_Dst.keras')\n",
    "except:\n",
    "    print(\"No saved model found, proceeding to train a new model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc4d1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_features, time_steps, units_gru, units_dense, dropout_rate, learning_rate):\n",
    "\n",
    "    \"\"\"Create model with configurable hyperparameters\"\"\"\n",
    "    model = Sequential([\n",
    "            # 0. Input Layer\n",
    "            Input(shape=(time_steps, n_features)),\n",
    "\n",
    "            # 1. Sequential Layer (GRU)\n",
    "            GRU(units=units_gru, activation='tanh', return_sequences=False),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            # 2. Classification Layer (Dense)\n",
    "            Dense(units=units_dense, activation='relu'),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            # 3. Output Layer\n",
    "            Dense(units=1, activation='linear')  # Linear activation for regression\n",
    "        ])\n",
    "        \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate), \n",
    "        loss='mae',\n",
    "        metrics=['mse', 'mae']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9f99f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_search(X_train, y_train, n_iter=20):\n",
    "    \"\"\"Random search over hyperparameters\"\"\"\n",
    "    \n",
    "    # Define hyperparameter ranges\n",
    "    param_distributions = {\n",
    "        'units_gru': [32, 64, 128, 256],\n",
    "        'units_dense': [16, 32, 64],\n",
    "        'dropout_rate': [0.1, 0.3, 0.5],\n",
    "        'learning_rate': [1e-3, 5e-4, 1e-4],\n",
    "        'time_steps': [3, 6, 12, 24],\n",
    "        'batch_size': [32, 64, 128]\n",
    "    }\n",
    "    \n",
    "    # support both 2D and 3D inputs\n",
    "    if X_train.ndim == 3:\n",
    "        n_samples, time_steps_from_X, n_features = X_train.shape\n",
    "    else:\n",
    "        n_samples, n_features = X_train.shape\n",
    "        time_steps_from_X = None\n",
    "    \n",
    "    # Compute class weights for imbalanced dataset\n",
    "    class_weights = dict(enumerate(compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )))\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Testing {n_iter} random combinations...\")\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Random sample from each parameter distribution\n",
    "        params = {\n",
    "            'units_gru': int(np.random.choice(param_distributions['units_gru'])),\n",
    "            'units_dense': int(np.random.choice(param_distributions['units_dense'])),\n",
    "            'dropout_rate': float(np.random.choice(param_distributions['dropout_rate'])),\n",
    "            'learning_rate': float(np.random.choice(param_distributions['learning_rate'])),\n",
    "            'time_steps': int(np.random.choice(param_distributions['time_steps'])),\n",
    "            'batch_size': int(np.random.choice(param_distributions['batch_size']))\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n[{i+1}/{n_iter}] Testing: {params}\")\n",
    "        \n",
    "        # If X_train already has time_steps dimension, ensure consistency\n",
    "        if time_steps_from_X is not None and params['time_steps'] != time_steps_from_X:\n",
    "            # skip incompatible combinations\n",
    "            print(f\"  -> Skipping params because X_train.time_steps={time_steps_from_X} != sampled time_steps={params['time_steps']}\")\n",
    "            continue\n",
    "        \n",
    "        # Create model (exclude batch_size from model creation)\n",
    "        model_params = {k: v for k, v in params.items() if k != 'batch_size'}\n",
    "        model = create_model(n_features=n_features, **model_params)\n",
    "        \n",
    "        es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=50,\n",
    "            batch_size=params['batch_size'],\n",
    "            callbacks=[es],\n",
    "            verbose=0,\n",
    "            class_weight=class_weights\n",
    "        )\n",
    "        \n",
    "        # Get best validation MSE\n",
    "        mse = max(history.history.get('mse', [0]))\n",
    "        print(f\"  -> mse: {mse:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({**params, 'mse': mse})\n",
    "        \n",
    "        # Update best\n",
    "        if mse > best_score:\n",
    "            best_score = mse\n",
    "            best_params = params\n",
    "    \n",
    "    print(\"Best Parameters:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"  Best Validation MSE: {best_score:.4f}\")\n",
    "    \n",
    "    return best_params, best_score, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd54b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 5 random combinations...\n",
      "\n",
      "[1/5] Testing: {'units_gru': 64, 'units_dense': 64, 'dropout_rate': 0.5, 'learning_rate': 0.0005, 'time_steps': 3, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "# Search for the best hyperparameters\n",
    "best_params, best_score, results = randomized_search(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    n_iter=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620637ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final model with best parameters\n",
    "model = Sequential([\n",
    "    Input(shape=(time_steps, n_features)),\n",
    "    GRU(best_params['units_gru'], activation='tanh'),\n",
    "    Dropout(best_params['dropout_rate']),\n",
    "    Dense(best_params['units_dense'], activation='relu'),\n",
    "    Dropout(best_params['dropout_rate']),\n",
    "    Dense(1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with best learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=best_params['learning_rate']),\n",
    "    loss='mae',\n",
    "    metrics=['mse', 'mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c0818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights for imbalanced dataset\n",
    "class_weights = dict(enumerate(compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70146f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2446/2446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - AUC: 0.9392 - Precision: 0.1769 - Recall: 0.8980 - loss: 0.3203 - val_AUC: 0.9451 - val_Precision: 0.2031 - val_Recall: 0.8332 - val_loss: 0.2381\n",
      "Epoch 2/10\n",
      "\u001b[1m2446/2446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - AUC: 0.9499 - Precision: 0.1904 - Recall: 0.9141 - loss: 0.2879 - val_AUC: 0.9446 - val_Precision: 0.1900 - val_Recall: 0.8587 - val_loss: 0.2752\n",
      "Epoch 3/10\n",
      "\u001b[1m2446/2446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - AUC: 0.9533 - Precision: 0.2009 - Recall: 0.9142 - loss: 0.2767 - val_AUC: 0.9411 - val_Precision: 0.1942 - val_Recall: 0.8192 - val_loss: 0.2221\n",
      "Epoch 4/10\n",
      "\u001b[1m2446/2446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - AUC: 0.9563 - Precision: 0.2047 - Recall: 0.9187 - loss: 0.2691 - val_AUC: 0.9458 - val_Precision: 0.1940 - val_Recall: 0.8587 - val_loss: 0.2305\n",
      "Epoch 5/10\n",
      "\u001b[1m2446/2446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - AUC: 0.9590 - Precision: 0.2122 - Recall: 0.9213 - loss: 0.2579 - val_AUC: 0.9423 - val_Precision: 0.1941 - val_Recall: 0.8447 - val_loss: 0.2270\n",
      "Epoch 6/10\n",
      "\u001b[1m2446/2446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - AUC: 0.9620 - Precision: 0.2156 - Recall: 0.9350 - loss: 0.2477 - val_AUC: 0.9411 - val_Precision: 0.1826 - val_Recall: 0.8496 - val_loss: 0.2379\n",
      "Epoch 7/10\n",
      "\u001b[1m2446/2446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - AUC: 0.9635 - Precision: 0.2162 - Recall: 0.9392 - loss: 0.2411 - val_AUC: 0.9365 - val_Precision: 0.2020 - val_Recall: 0.7855 - val_loss: 0.2175\n",
      "Epoch 8/10\n",
      "\u001b[1m2446/2446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - AUC: 0.9652 - Precision: 0.2220 - Recall: 0.9395 - loss: 0.2354 - val_AUC: 0.9371 - val_Precision: 0.1729 - val_Recall: 0.8291 - val_loss: 0.2480\n",
      "Epoch 9/10\n",
      "\u001b[1m2446/2446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - AUC: 0.9678 - Precision: 0.2330 - Recall: 0.9422 - loss: 0.2251 - val_AUC: 0.9397 - val_Precision: 0.1887 - val_Recall: 0.8398 - val_loss: 0.2450\n",
      "Epoch 10/10\n",
      "\u001b[1m2446/2446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - AUC: 0.9694 - Precision: 0.2370 - Recall: 0.9474 - loss: 0.2177 - val_AUC: 0.9356 - val_Precision: 0.2157 - val_Recall: 0.7716 - val_loss: 0.1959\n"
     ]
    }
   ],
   "source": [
    "# Train the best model\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    callbacks=[es],\n",
    "    verbose=1,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21ce9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.23334245383739471, 'compile_metrics': 0.16242630779743195}\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(dict(zip(model.metrics_names, results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8e904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ../data/storms_keras_RNN.keras\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "model.save(\"../data/storms_keras_RNN_Dst.keras\")\n",
    "print(\"Best model saved to ../data/storms_keras_RNN_Dst.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c6c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1529/1529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)       # threshold at 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# un-scale final predictions before calculating the final metrics\n",
    "\n",
    "# 1. Reshape scaled true values for inverse transformation\n",
    "# y_test_scaled is usually 1D, but the scaler expects 2D\n",
    "y_test_scaled_2d = y_test.reshape(-1, 1)\n",
    "\n",
    "# 2. Inverse Transform the predictions and the true values\n",
    "# This converts the values back into the original Dst units (nT)\n",
    "y_pred_unscaled = y_scaler.inverse_transform(y_pred)\n",
    "y_true_unscaled = y_scaler.inverse_transform(y_test_scaled_2d)\n",
    "\n",
    "# 3. Flatten the unscaled arrays to 1D for metric calculation (optional but clean)\n",
    "y_pred_unscaled = y_pred_unscaled.flatten()\n",
    "y_true_unscaled = y_true_unscaled.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5632a",
   "metadata": {},
   "source": [
    "# Results\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9049289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[42295  5399]\n",
      " [  166  1047]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94     47694\n",
      "           1       0.16      0.86      0.27      1213\n",
      "\n",
      "    accuracy                           0.89     48907\n",
      "   macro avg       0.58      0.87      0.61     48907\n",
      "weighted avg       0.98      0.89      0.92     48907\n",
      "\n",
      "\n",
      "Evaluation Metrics:\n",
      "       Metric  Score\n",
      "           F2 0.4634\n",
      "           F1 0.2734\n",
      "       Recall 0.8631\n",
      "Avg Precision 0.4977\n",
      "      ROC AUC 0.9491\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "if len(np.unique(y_test)) > 1:\n",
    "    final_mae = mean_absolute_error(y_true_unscaled, y_pred_unscaled)\n",
    "    final_rmse = np.sqrt(mean_squared_error(y_true_unscaled, y_pred_unscaled))\n",
    "\n",
    "    metrics_table = pd.DataFrame({\n",
    "        'Metric': ['MAE', 'rmse'],\n",
    "        'Score': [final_mae, final_rmse]\n",
    "    })\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(metrics_table.to_string(index=False, float_format=\"%.4f\"))\n",
    "else:\n",
    "    print(\"Only one class present in y_test. Metrics skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b82df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold Analysis:\n",
      "\n",
      "Threshold = 0.3\n",
      "  TP: 1119, FP: 7846, TN: 39848, FN: 94\n",
      "  Precision: 0.125, Recall: 0.923\n",
      "\n",
      "Threshold = 0.4\n",
      "  TP: 1078, FP: 6610, TN: 41084, FN: 135\n",
      "  Precision: 0.140, Recall: 0.889\n",
      "\n",
      "Threshold = 0.5\n",
      "  TP: 1047, FP: 5399, TN: 42295, FN: 166\n",
      "  Precision: 0.162, Recall: 0.863\n",
      "\n",
      "Threshold = 0.6\n",
      "  TP: 991, FP: 4282, TN: 43412, FN: 222\n",
      "  Precision: 0.188, Recall: 0.817\n",
      "\n",
      "Threshold = 0.7\n",
      "  TP: 905, FP: 3034, TN: 44660, FN: 308\n",
      "  Precision: 0.230, Recall: 0.746\n"
     ]
    }
   ],
   "source": [
    "# Analyze different classification thresholds\n",
    "print(\"\\nThreshold Analysis:\")\n",
    "\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba >= thresh).astype(int)\n",
    "    cm_thresh = confusion_matrix(y_test, y_pred_thresh)\n",
    "    tn, fp, fn, tp = cm_thresh.ravel() if cm_thresh.size == 4 else (cm_thresh[0,0], 0, 0, 0)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nThreshold = {thresh}\")\n",
    "    print(f\"  TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "    print(f\"  Precision: {precision:.3f}, Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "\n",
    "# Calculate the residuals/errors\n",
    "residuals = y_true_unscaled - y_pred_unscaled\n",
    "abs_errors = np.abs(residuals)\n",
    "\n",
    "# Calculate metrics for display\n",
    "mae = mean_absolute_error(y_true_unscaled, y_pred_unscaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_true_unscaled, y_pred_unscaled))\n",
    "\n",
    "\n",
    "# --- 2. PLOTTING FUNCTION ---\n",
    "def plot_regression_diagnostics(y_true, y_pred, mae, rmse):\n",
    "    \"\"\"\n",
    "    Creates a 2x2 figure containing the four key regression diagnostic plots\n",
    "    for the Dst index forecast.\n",
    "    \"\"\"\n",
    "    \n",
    "    residuals = y_true - y_pred\n",
    "    abs_errors = np.abs(residuals)\n",
    "    \n",
    "    # Set the plotting style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Create the figure and subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    plt.suptitle(\n",
    "        f\"Geomagnetic Dst Regression Model Diagnostics (MAE: {mae:.2f} nT, RMSE: {rmse:.2f} nT)\", \n",
    "        fontsize=18, \n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "    # --- Plot 1: Time-Series Plot (Predicted vs. True) ---\n",
    "    ax1 = axes[0, 0]\n",
    "    # Restrict the view to a short, meaningful window (e.g., 200 data points)\n",
    "    window_size = 200\n",
    "    ax1.plot(y_true[:window_size], label='True Dst (nT)', color='#004D99', linewidth=2)\n",
    "    ax1.plot(y_pred[:window_size], label='Predicted Dst (nT)', color='#FF7F00', linestyle='--', linewidth=1.5)\n",
    "    \n",
    "    ax1.set_title(f'1. Time-Series Plot (Subset of {window_size} Hours)', fontsize=14)\n",
    "    ax1.set_xlabel('Time Step (Hours)')\n",
    "    ax1.set_ylabel('Dst Index (nT)')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Highlight the storm region (Dst < -50 nT)\n",
    "    ax1.axhline(y=-50, color='r', linestyle=':', alpha=0.7, label='Storm Threshold')\n",
    "    \n",
    "    # --- Plot 2: Scatter Plot (Predicted vs. True) ---\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    # Use hexbin for dense data to show density/clusters\n",
    "    ax2.hexbin(y_true, y_pred, gridsize=40, cmap='viridis', mincnt=1)\n",
    "    \n",
    "    # Plot the perfect prediction line (y=x)\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    ax2.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction', alpha=0.7)\n",
    "    \n",
    "    ax2.set_title('2. Predicted vs. True Dst (nT)', fontsize=14)\n",
    "    ax2.set_xlabel('True Dst (nT)')\n",
    "    ax2.set_ylabel('Predicted Dst (nT)')\n",
    "    ax2.legend()\n",
    "    ax2.set_aspect('equal', adjustable='box')\n",
    "\n",
    "\n",
    "    # --- Plot 3: Error Distribution (Histogram) ---\n",
    "    ax3 = axes[1, 0]\n",
    "    # Use seaborn for a nice distribution plot\n",
    "    sns.histplot(residuals, bins=50, kde=True, ax=ax3, color='#2CA02C', edgecolor='black')\n",
    "    \n",
    "    # Add a line at zero error\n",
    "    ax3.axvline(0, color='r', linestyle='--', linewidth=1.5, label='Zero Error')\n",
    "    \n",
    "    ax3.set_title('3. Residuals (Error) Distribution', fontsize=14)\n",
    "    ax3.set_xlabel('Residual (True Dst - Predicted Dst) [nT]')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.legend()\n",
    "\n",
    "\n",
    "    # --- Plot 4: Error Magnitude vs. Dst Value ---\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.scatter(y_true, abs_errors, s=5, alpha=0.5, color='#1F77B4')\n",
    "    \n",
    "    # Add line showing the mean absolute error\n",
    "    ax4.axhline(mae, color='red', linestyle='-', linewidth=2, label=f'Mean Abs. Error ({mae:.2f})')\n",
    "    \n",
    "    ax4.set_title('4. Absolute Error vs. True Dst Value', fontsize=14)\n",
    "    ax4.set_xlabel('True Dst Index (nT)')\n",
    "    ax4.set_ylabel('Absolute Error |True - Predicted| (nT)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Final layout adjustments\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust to make space for suptitle\n",
    "    plt.savefig('../figures/geomag_storm_RNN_Dst_results.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nVisualization saved as 'geomag_storm_RNN_Dst_results.png'\")\n",
    "    plt.show()\n",
    "\n",
    "# --- 3. EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "    plot_regression_diagnostics(y_true_unscaled, y_pred_unscaled, mae, rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
