{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc66fcb",
   "metadata": {},
   "source": [
    "# Forecasting geomagnetic storms using support vector machine for classification\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcde7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                             roc_curve, precision_recall_curve, average_precision_score, \n",
    "                             fbeta_score, recall_score, make_scorer)\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15a00a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244535 entries, 0 to 244534\n",
      "Data columns (total 19 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   datetime     244535 non-null  object \n",
      " 1   ABS_B        244535 non-null  float64\n",
      " 2   F            244535 non-null  float64\n",
      " 3   BX_GSE       244535 non-null  float64\n",
      " 4   BY_GSE       244535 non-null  float64\n",
      " 5   BZ_GSE       244535 non-null  float64\n",
      " 6   SIGMA-ABS_B  244535 non-null  float64\n",
      " 7   SIGMA-B      244535 non-null  float64\n",
      " 8   SIGMA-Bx     244535 non-null  float64\n",
      " 9   SIGMA-By     244535 non-null  float64\n",
      " 10  SIGMA-Bz     244535 non-null  float64\n",
      " 11  T            244535 non-null  float64\n",
      " 12  N            244535 non-null  float64\n",
      " 13  V            244535 non-null  float64\n",
      " 14  Ratio        244535 non-null  float64\n",
      " 15  Pressure     244535 non-null  float64\n",
      " 16  R            244535 non-null  int64  \n",
      " 17  DST          244535 non-null  float64\n",
      " 18  storm_now    244535 non-null  int64  \n",
      "dtypes: float64(16), int64(2), object(1)\n",
      "memory usage: 35.4+ MB\n",
      "None\n",
      "(244535, 19)\n",
      "              datetime  ABS_B    F  BX_GSE  BY_GSE  BZ_GSE  SIGMA-ABS_B  \\\n",
      "0  1995-01-01 00:00:00    4.0  3.9    -0.6     3.2    -2.1          0.6   \n",
      "1  1995-01-01 01:00:00    3.0  2.9    -0.9     2.4    -1.3          0.3   \n",
      "2  1995-01-01 02:00:00    3.2  1.9     0.5     1.8    -0.4          0.7   \n",
      "3  1995-01-01 03:00:00    4.3  4.0    -3.2     2.5     0.3          0.4   \n",
      "4  1995-01-01 04:00:00    4.8  4.8    -4.3     2.1     0.3          0.1   \n",
      "\n",
      "   SIGMA-B  SIGMA-Bx  SIGMA-By  SIGMA-Bz        T     N      V  Ratio  \\\n",
      "0      1.2       0.5       0.9       0.5  15816.0  16.3  315.0  0.016   \n",
      "1      0.8       0.2       0.3       0.8  15601.0  18.8  315.0  0.013   \n",
      "2      2.6       1.6       0.9       1.8  20703.0  19.3  320.0  0.012   \n",
      "3      1.5       1.2       0.5       0.8  25094.0  16.4  317.0  0.013   \n",
      "4      0.3       0.2       0.2       0.2  25604.0  14.5  313.0  0.015   \n",
      "\n",
      "   Pressure   R  DST  storm_now  \n",
      "0      2.87  13 -2.0          0  \n",
      "1      3.28  13  3.0          0  \n",
      "2      3.46  13  6.0          0  \n",
      "3      2.90  13  5.0          0  \n",
      "4      2.51  13  3.0          0  \n"
     ]
    }
   ],
   "source": [
    "# Load raw dataframe\n",
    "df = pd.read_csv(\"../data/data_storms.csv\")\n",
    "\n",
    "print(df.info())\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f77663",
   "metadata": {},
   "source": [
    "# Changing the dataframe to fit SVM\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9995b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244535 entries, 0 to 244534\n",
      "Data columns (total 19 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   datetime     244535 non-null  datetime64[ns]\n",
      " 1   ABS_B        244535 non-null  float64       \n",
      " 2   F            244535 non-null  float64       \n",
      " 3   BX_GSE       244535 non-null  float64       \n",
      " 4   BY_GSE       244535 non-null  float64       \n",
      " 5   BZ_GSE       244535 non-null  float64       \n",
      " 6   SIGMA-ABS_B  244535 non-null  float64       \n",
      " 7   SIGMA-B      244535 non-null  float64       \n",
      " 8   SIGMA-Bx     244535 non-null  float64       \n",
      " 9   SIGMA-By     244535 non-null  float64       \n",
      " 10  SIGMA-Bz     244535 non-null  float64       \n",
      " 11  T            244535 non-null  float64       \n",
      " 12  N            244535 non-null  float64       \n",
      " 13  V            244535 non-null  float64       \n",
      " 14  Ratio        244535 non-null  float64       \n",
      " 15  Pressure     244535 non-null  float64       \n",
      " 16  R            244535 non-null  int64         \n",
      " 17  DST          244535 non-null  float64       \n",
      " 18  storm_now    244535 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(16), int64(2)\n",
      "memory usage: 35.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'datetime' column is in datetime format\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "020c4fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sum\n",
      "datetime     \n",
      "1995      371\n",
      "1996       31\n",
      "1997      284\n",
      "1998      555\n",
      "1999      424\n",
      "2000      554\n",
      "2001      677\n",
      "2002      904\n",
      "2003      335\n",
      "2004      354\n",
      "2005      533\n",
      "2006       93\n",
      "2007       20\n",
      "2008       12\n",
      "2009       17\n",
      "2010       84\n",
      "2011      201\n",
      "2012      425\n",
      "2013      224\n",
      "2014      163\n",
      "2015      659\n",
      "2016      219\n",
      "2017      213\n",
      "2018       80\n",
      "2019       24\n",
      "2020       21\n",
      "2021       51\n",
      "2022      159\n",
      "2023      386\n",
      "2024      572\n"
     ]
    }
   ],
   "source": [
    "# Analyze storm distribution over time\n",
    "\n",
    "# Number of storm-hours per year\n",
    "df_storms = df.groupby(df['datetime'].dt.year)['storm_now'].agg(['sum'])\n",
    "print(df_storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b756e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hours with storm distribution by year:\n",
      "          sum  count\n",
      "datetime            \n",
      "1995      371   8628\n",
      "1996       31   8578\n",
      "1997      284   8121\n",
      "1998      555   8328\n",
      "1999      424   6840\n",
      "2000      554   7034\n",
      "2001      677   7466\n",
      "2002      904   7613\n",
      "2003      335   6439\n",
      "2004      354   6948\n",
      "2005      533   8508\n",
      "2006       93   7723\n",
      "2007       20   8230\n",
      "2008       12   8649\n",
      "2009       17   8550\n",
      "2010       84   7966\n",
      "2011      201   8313\n",
      "2012      425   8651\n",
      "2013      224   8412\n",
      "2014      163   6897\n",
      "2015      659   8670\n",
      "2016      219   8741\n",
      "2017      213   8712\n",
      "2018       80   8751\n",
      "2019       24   8643\n",
      "2020       21   8722\n",
      "2021       51   8549\n",
      "2022      159   8716\n",
      "2023      386   8614\n",
      "2024      572   8523\n"
     ]
    }
   ],
   "source": [
    "# Number of hours with storms (count should be 8760 [24*365] or 8784 for leap years)\n",
    "yearly_storms = df.groupby(df['datetime'].dt.year)['storm_now'].agg(['sum', 'count'])\n",
    "print(\"\\nHours with storm distribution by year:\")\n",
    "print(yearly_storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f7bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_features(df, n_hours=3):\n",
    "    \"\"\"\n",
    "    Create features using the last n_hours of data to predict the next hour's storm.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    # Features to use (exclude datetime and target)\n",
    "    feature_cols = [col for col in df.columns if col not in ['datetime', 'storm_now']]\n",
    "    \n",
    "    # Create lagged features for each of the past n_hours\n",
    "    lagged_df = pd.DataFrame(index=df.index) # important to keep the index for alignment\n",
    "    \n",
    "    for i in range(1, n_hours + 1):\n",
    "        for col in feature_cols:\n",
    "            lagged_df[f'{col}_lag{i}'] = df[col].shift(i)\n",
    "    \n",
    "    # Target: next hour's storm (shift storm_now by -1)\n",
    "    lagged_df['target'] = df['storm_now'].shift(-1)\n",
    "    lagged_df['datetime'] = df['datetime']\n",
    "    \n",
    "    # Drop rows with NaN (first n_hours rows and last row)\n",
    "    lagged_df = lagged_df.dropna()\n",
    "    \n",
    "    return lagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d513715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 244531 entries, 3 to 244533\n",
      "Data columns (total 53 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   ABS_B_lag1        244531 non-null  float64       \n",
      " 1   F_lag1            244531 non-null  float64       \n",
      " 2   BX_GSE_lag1       244531 non-null  float64       \n",
      " 3   BY_GSE_lag1       244531 non-null  float64       \n",
      " 4   BZ_GSE_lag1       244531 non-null  float64       \n",
      " 5   SIGMA-ABS_B_lag1  244531 non-null  float64       \n",
      " 6   SIGMA-B_lag1      244531 non-null  float64       \n",
      " 7   SIGMA-Bx_lag1     244531 non-null  float64       \n",
      " 8   SIGMA-By_lag1     244531 non-null  float64       \n",
      " 9   SIGMA-Bz_lag1     244531 non-null  float64       \n",
      " 10  T_lag1            244531 non-null  float64       \n",
      " 11  N_lag1            244531 non-null  float64       \n",
      " 12  V_lag1            244531 non-null  float64       \n",
      " 13  Ratio_lag1        244531 non-null  float64       \n",
      " 14  Pressure_lag1     244531 non-null  float64       \n",
      " 15  R_lag1            244531 non-null  float64       \n",
      " 16  DST_lag1          244531 non-null  float64       \n",
      " 17  ABS_B_lag2        244531 non-null  float64       \n",
      " 18  F_lag2            244531 non-null  float64       \n",
      " 19  BX_GSE_lag2       244531 non-null  float64       \n",
      " 20  BY_GSE_lag2       244531 non-null  float64       \n",
      " 21  BZ_GSE_lag2       244531 non-null  float64       \n",
      " 22  SIGMA-ABS_B_lag2  244531 non-null  float64       \n",
      " 23  SIGMA-B_lag2      244531 non-null  float64       \n",
      " 24  SIGMA-Bx_lag2     244531 non-null  float64       \n",
      " 25  SIGMA-By_lag2     244531 non-null  float64       \n",
      " 26  SIGMA-Bz_lag2     244531 non-null  float64       \n",
      " 27  T_lag2            244531 non-null  float64       \n",
      " 28  N_lag2            244531 non-null  float64       \n",
      " 29  V_lag2            244531 non-null  float64       \n",
      " 30  Ratio_lag2        244531 non-null  float64       \n",
      " 31  Pressure_lag2     244531 non-null  float64       \n",
      " 32  R_lag2            244531 non-null  float64       \n",
      " 33  DST_lag2          244531 non-null  float64       \n",
      " 34  ABS_B_lag3        244531 non-null  float64       \n",
      " 35  F_lag3            244531 non-null  float64       \n",
      " 36  BX_GSE_lag3       244531 non-null  float64       \n",
      " 37  BY_GSE_lag3       244531 non-null  float64       \n",
      " 38  BZ_GSE_lag3       244531 non-null  float64       \n",
      " 39  SIGMA-ABS_B_lag3  244531 non-null  float64       \n",
      " 40  SIGMA-B_lag3      244531 non-null  float64       \n",
      " 41  SIGMA-Bx_lag3     244531 non-null  float64       \n",
      " 42  SIGMA-By_lag3     244531 non-null  float64       \n",
      " 43  SIGMA-Bz_lag3     244531 non-null  float64       \n",
      " 44  T_lag3            244531 non-null  float64       \n",
      " 45  N_lag3            244531 non-null  float64       \n",
      " 46  V_lag3            244531 non-null  float64       \n",
      " 47  Ratio_lag3        244531 non-null  float64       \n",
      " 48  Pressure_lag3     244531 non-null  float64       \n",
      " 49  R_lag3            244531 non-null  float64       \n",
      " 50  DST_lag3          244531 non-null  float64       \n",
      " 51  target            244531 non-null  float64       \n",
      " 52  datetime          244531 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(52)\n",
      "memory usage: 100.7 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the lagged features for the past 3 hours\n",
    "lagged_data = create_lagged_features(df, n_hours=3)\n",
    "\n",
    "print(lagged_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "160f1873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 24453 entries, 7755 to 53625\n",
      "Data columns (total 53 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   ABS_B_lag1        24453 non-null  float64       \n",
      " 1   F_lag1            24453 non-null  float64       \n",
      " 2   BX_GSE_lag1       24453 non-null  float64       \n",
      " 3   BY_GSE_lag1       24453 non-null  float64       \n",
      " 4   BZ_GSE_lag1       24453 non-null  float64       \n",
      " 5   SIGMA-ABS_B_lag1  24453 non-null  float64       \n",
      " 6   SIGMA-B_lag1      24453 non-null  float64       \n",
      " 7   SIGMA-Bx_lag1     24453 non-null  float64       \n",
      " 8   SIGMA-By_lag1     24453 non-null  float64       \n",
      " 9   SIGMA-Bz_lag1     24453 non-null  float64       \n",
      " 10  T_lag1            24453 non-null  float64       \n",
      " 11  N_lag1            24453 non-null  float64       \n",
      " 12  V_lag1            24453 non-null  float64       \n",
      " 13  Ratio_lag1        24453 non-null  float64       \n",
      " 14  Pressure_lag1     24453 non-null  float64       \n",
      " 15  R_lag1            24453 non-null  float64       \n",
      " 16  DST_lag1          24453 non-null  float64       \n",
      " 17  ABS_B_lag2        24453 non-null  float64       \n",
      " 18  F_lag2            24453 non-null  float64       \n",
      " 19  BX_GSE_lag2       24453 non-null  float64       \n",
      " 20  BY_GSE_lag2       24453 non-null  float64       \n",
      " 21  BZ_GSE_lag2       24453 non-null  float64       \n",
      " 22  SIGMA-ABS_B_lag2  24453 non-null  float64       \n",
      " 23  SIGMA-B_lag2      24453 non-null  float64       \n",
      " 24  SIGMA-Bx_lag2     24453 non-null  float64       \n",
      " 25  SIGMA-By_lag2     24453 non-null  float64       \n",
      " 26  SIGMA-Bz_lag2     24453 non-null  float64       \n",
      " 27  T_lag2            24453 non-null  float64       \n",
      " 28  N_lag2            24453 non-null  float64       \n",
      " 29  V_lag2            24453 non-null  float64       \n",
      " 30  Ratio_lag2        24453 non-null  float64       \n",
      " 31  Pressure_lag2     24453 non-null  float64       \n",
      " 32  R_lag2            24453 non-null  float64       \n",
      " 33  DST_lag2          24453 non-null  float64       \n",
      " 34  ABS_B_lag3        24453 non-null  float64       \n",
      " 35  F_lag3            24453 non-null  float64       \n",
      " 36  BX_GSE_lag3       24453 non-null  float64       \n",
      " 37  BY_GSE_lag3       24453 non-null  float64       \n",
      " 38  BZ_GSE_lag3       24453 non-null  float64       \n",
      " 39  SIGMA-ABS_B_lag3  24453 non-null  float64       \n",
      " 40  SIGMA-B_lag3      24453 non-null  float64       \n",
      " 41  SIGMA-Bx_lag3     24453 non-null  float64       \n",
      " 42  SIGMA-By_lag3     24453 non-null  float64       \n",
      " 43  SIGMA-Bz_lag3     24453 non-null  float64       \n",
      " 44  T_lag3            24453 non-null  float64       \n",
      " 45  N_lag3            24453 non-null  float64       \n",
      " 46  V_lag3            24453 non-null  float64       \n",
      " 47  Ratio_lag3        24453 non-null  float64       \n",
      " 48  Pressure_lag3     24453 non-null  float64       \n",
      " 49  R_lag3            24453 non-null  float64       \n",
      " 50  DST_lag3          24453 non-null  float64       \n",
      " 51  target            24453 non-null  float64       \n",
      " 52  datetime          24453 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(52)\n",
      "memory usage: 10.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# The dataset is too big to run the hyperparameter opitmization in my PC, so I am going to use a sample for this part\n",
    "lagged_data_short = lagged_data.sample(frac=0.10)\n",
    "print(lagged_data_short.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81f5298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ABS_B_lag1  F_lag1  BX_GSE_lag1  BY_GSE_lag1  BZ_GSE_lag1  \\\n",
      "0         4.3     4.0         -3.2          2.5          0.3   \n",
      "1         4.2     3.5         -1.7          3.0          0.7   \n",
      "2        13.9    13.2          0.7         -6.7        -11.4   \n",
      "3        12.2    11.9          6.7         -9.8          0.5   \n",
      "4         5.7     4.6          2.7         -3.7          0.7   \n",
      "\n",
      "   SIGMA-ABS_B_lag1  SIGMA-B_lag1  SIGMA-Bx_lag1  SIGMA-By_lag1  \\\n",
      "0               0.4           1.5            1.2            0.5   \n",
      "1               0.3           2.1            1.1            0.7   \n",
      "2               1.4           4.4            1.3            3.5   \n",
      "3               0.5           2.4            0.9            1.2   \n",
      "4               0.4           3.0            1.5            1.2   \n",
      "\n",
      "   SIGMA-Bz_lag1  ...  SIGMA-Bz_lag3    T_lag3  N_lag3  V_lag3  Ratio_lag3  \\\n",
      "0            0.8  ...            0.8   15601.0    18.8   315.0       0.013   \n",
      "1            1.7  ...            0.3   24108.0    16.0   315.0       0.014   \n",
      "2            2.3  ...            1.9  344444.0     4.8   500.0       0.047   \n",
      "3            1.8  ...            3.1  263596.0     5.7   489.0       0.045   \n",
      "4            2.3  ...            1.4  188094.0     3.2   683.0       0.041   \n",
      "\n",
      "   Pressure_lag3  R_lag3  DST_lag3  target            datetime  \n",
      "0           3.28    13.0       3.0     0.0 1995-01-01 04:00:00  \n",
      "1           2.80    13.0       4.0     0.0 1995-01-01 08:00:00  \n",
      "2           2.38    11.0     -17.0     0.0 1995-01-02 18:00:00  \n",
      "3           2.69    11.0     -38.0     0.0 1995-01-02 21:00:00  \n",
      "4           2.90    19.0     -25.0     0.0 1995-01-04 05:00:00  \n",
      "\n",
      "[5 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "# chronological split is important for time series\n",
    "lagged_data_short = lagged_data_short.sort_values('datetime').reset_index(drop=True)\n",
    "assert lagged_data_short['datetime'].is_monotonic_increasing\n",
    "\n",
    "print(lagged_data_short.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21b447ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ABS_B_lag1', 'F_lag1', 'BX_GSE_lag1', 'BY_GSE_lag1', 'BZ_GSE_lag1',\n",
      "       'SIGMA-ABS_B_lag1', 'SIGMA-B_lag1', 'SIGMA-Bx_lag1', 'SIGMA-By_lag1',\n",
      "       'SIGMA-Bz_lag1', 'T_lag1', 'N_lag1', 'V_lag1', 'Ratio_lag1',\n",
      "       'Pressure_lag1', 'R_lag1', 'DST_lag1', 'ABS_B_lag2', 'F_lag2',\n",
      "       'BX_GSE_lag2', 'BY_GSE_lag2', 'BZ_GSE_lag2', 'SIGMA-ABS_B_lag2',\n",
      "       'SIGMA-B_lag2', 'SIGMA-Bx_lag2', 'SIGMA-By_lag2', 'SIGMA-Bz_lag2',\n",
      "       'T_lag2', 'N_lag2', 'V_lag2', 'Ratio_lag2', 'Pressure_lag2', 'R_lag2',\n",
      "       'DST_lag2', 'ABS_B_lag3', 'F_lag3', 'BX_GSE_lag3', 'BY_GSE_lag3',\n",
      "       'BZ_GSE_lag3', 'SIGMA-ABS_B_lag3', 'SIGMA-B_lag3', 'SIGMA-Bx_lag3',\n",
      "       'SIGMA-By_lag3', 'SIGMA-Bz_lag3', 'T_lag3', 'N_lag3', 'V_lag3',\n",
      "       'Ratio_lag3', 'Pressure_lag3', 'R_lag3', 'DST_lag3', 'target',\n",
      "       'datetime'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# let's check all columns to dorp some on the next step\n",
    "print(lagged_data_short.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dba8938",
   "metadata": {},
   "source": [
    "# Fitting the model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82f4dcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24453 entries, 0 to 24452\n",
      "Data columns (total 51 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   ABS_B_lag1        24453 non-null  float64\n",
      " 1   F_lag1            24453 non-null  float64\n",
      " 2   BX_GSE_lag1       24453 non-null  float64\n",
      " 3   BY_GSE_lag1       24453 non-null  float64\n",
      " 4   BZ_GSE_lag1       24453 non-null  float64\n",
      " 5   SIGMA-ABS_B_lag1  24453 non-null  float64\n",
      " 6   SIGMA-B_lag1      24453 non-null  float64\n",
      " 7   SIGMA-Bx_lag1     24453 non-null  float64\n",
      " 8   SIGMA-By_lag1     24453 non-null  float64\n",
      " 9   SIGMA-Bz_lag1     24453 non-null  float64\n",
      " 10  T_lag1            24453 non-null  float64\n",
      " 11  N_lag1            24453 non-null  float64\n",
      " 12  V_lag1            24453 non-null  float64\n",
      " 13  Ratio_lag1        24453 non-null  float64\n",
      " 14  Pressure_lag1     24453 non-null  float64\n",
      " 15  R_lag1            24453 non-null  float64\n",
      " 16  DST_lag1          24453 non-null  float64\n",
      " 17  ABS_B_lag2        24453 non-null  float64\n",
      " 18  F_lag2            24453 non-null  float64\n",
      " 19  BX_GSE_lag2       24453 non-null  float64\n",
      " 20  BY_GSE_lag2       24453 non-null  float64\n",
      " 21  BZ_GSE_lag2       24453 non-null  float64\n",
      " 22  SIGMA-ABS_B_lag2  24453 non-null  float64\n",
      " 23  SIGMA-B_lag2      24453 non-null  float64\n",
      " 24  SIGMA-Bx_lag2     24453 non-null  float64\n",
      " 25  SIGMA-By_lag2     24453 non-null  float64\n",
      " 26  SIGMA-Bz_lag2     24453 non-null  float64\n",
      " 27  T_lag2            24453 non-null  float64\n",
      " 28  N_lag2            24453 non-null  float64\n",
      " 29  V_lag2            24453 non-null  float64\n",
      " 30  Ratio_lag2        24453 non-null  float64\n",
      " 31  Pressure_lag2     24453 non-null  float64\n",
      " 32  R_lag2            24453 non-null  float64\n",
      " 33  DST_lag2          24453 non-null  float64\n",
      " 34  ABS_B_lag3        24453 non-null  float64\n",
      " 35  F_lag3            24453 non-null  float64\n",
      " 36  BX_GSE_lag3       24453 non-null  float64\n",
      " 37  BY_GSE_lag3       24453 non-null  float64\n",
      " 38  BZ_GSE_lag3       24453 non-null  float64\n",
      " 39  SIGMA-ABS_B_lag3  24453 non-null  float64\n",
      " 40  SIGMA-B_lag3      24453 non-null  float64\n",
      " 41  SIGMA-Bx_lag3     24453 non-null  float64\n",
      " 42  SIGMA-By_lag3     24453 non-null  float64\n",
      " 43  SIGMA-Bz_lag3     24453 non-null  float64\n",
      " 44  T_lag3            24453 non-null  float64\n",
      " 45  N_lag3            24453 non-null  float64\n",
      " 46  V_lag3            24453 non-null  float64\n",
      " 47  Ratio_lag3        24453 non-null  float64\n",
      " 48  Pressure_lag3     24453 non-null  float64\n",
      " 49  R_lag3            24453 non-null  float64\n",
      " 50  DST_lag3          24453 non-null  float64\n",
      "dtypes: float64(51)\n",
      "memory usage: 9.5 MB\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 24453 entries, 0 to 24452\n",
      "Series name: target\n",
      "Non-Null Count  Dtype  \n",
      "--------------  -----  \n",
      "24453 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 191.2 KB\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "# I started using the DST feature, and it is be very useful, but it is not always available in real-time forecasts\n",
    "#X = lagged_data_short.drop(['datetime', 'target', 'DST_lag1', 'DST_lag2', 'DST_lag3'], axis=1)\n",
    "X = lagged_data_short.drop(['datetime', 'target'], axis=1)\n",
    "y = lagged_data_short['target']\n",
    "\n",
    "print(X.info(), y.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978fe000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training samples: 19562\n",
      "Test samples: 4891\n",
      "\n",
      "Class distribution in training set:\n",
      "target\n",
      "0.0    18833\n",
      "1.0      729\n",
      "Name: count, dtype: int64\n",
      "Percentage of positive cases in train: 3.73%\n",
      "\n",
      "Class distribution in test set:\n",
      "target\n",
      "0.0    4759\n",
      "1.0     132\n",
      "Name: count, dtype: int64\n",
      "Percentage of positive cases in test: 2.70%\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets\n",
    "\n",
    "# Use last 20% as test set\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"Percentage of positive cases in train: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"\\nClass distribution in test set:\")\n",
    "print(y_test.value_counts())\n",
    "print(f\"Percentage of positive cases in test: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68eadce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring metric\n",
    "scoring = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08597d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TimeSeriesSplit for cross-validation in time series data\n",
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b90711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "# GridSearchCV with balanced and umbalanced data\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    #('smote', SMOTE(random_state=42)),\n",
    "    (\"svm\", SVC(class_weight='balanced', probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = [\n",
    "    {'svm__kernel': ['linear'], 'svm__C': [0.1, 1, 10, 100]},\n",
    "    {'svm__kernel': ['rbf'], 'svm__C': [0.1, 1, 10, 100], 'svm__gamma': ['scale', 0.01, 0.001]},\n",
    "    {'svm__kernel': ['poly'], 'svm__C': [0.1, 1, 10, 100], 'svm__gamma': ['scale', 0.01, 0.001]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9e0b119",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data/storms_best_SVM.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# you can just load the model if already trained and not run the grid search and best model again\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# in that case, do not run the next three cells\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m best_svm = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./Data/storms_best_SVM.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ml:\\Git\\hbr-solar-forecast-project\\ml_env\\Lib\\site-packages\\joblib\\numpy_pickle.py:735\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    733\u001b[39m         obj = _unpickle(fobj, ensure_native_byte_order=ensure_native_byte_order)\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _validate_fileobject_and_memmap(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m    737\u001b[39m             fobj,\n\u001b[32m    738\u001b[39m             validated_mmap_mode,\n\u001b[32m    739\u001b[39m         ):\n\u001b[32m    740\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    741\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    742\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    743\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './Data/storms_best_SVM.pkl'"
     ]
    }
   ],
   "source": [
    "# you can just load the model if already trained and not run the grid search and best model again\n",
    "# in that case, do not run the next three cells\n",
    "best_svm = joblib.load(\"./Data/storms_best_SVM.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86602a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the best hyperparameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting grid search for SVM (this may take a while)...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee6c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, I am going to use the full dataset to train the final model using the best hyperparameters found\n",
    "\n",
    "lagged_data = lagged_data.sort_values('datetime').reset_index(drop=True)\n",
    "assert lagged_data['datetime'].is_monotonic_increasing\n",
    "\n",
    "# I tested using and not using the DST feature, and the results were better with it\n",
    "#X_full = lagged_data.drop(['datetime', 'target', 'DST_lag1', 'DST_lag2', 'DST_lag3'], axis=1)\n",
    "X_full = lagged_data.drop(['datetime', 'target'], axis=1)\n",
    "y_full = lagged_data['target']\n",
    "\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train_full, X_test_full = X_full.iloc[:split_idx], X_full.iloc[split_idx:]\n",
    "y_train_full, y_test_full = y_full.iloc[:split_idx], y_full.iloc[split_idx:]\n",
    "\n",
    "scaler_full = StandardScaler()\n",
    "X_train_full_scaled = scaler_full.fit_transform(X_train_full)\n",
    "X_test_full_scaled = scaler_full.transform(X_test_full)\n",
    "\n",
    "#smote_full = SMOTE(random_state=42)\n",
    "#X_train_full_balanced, y_train_full_balanced = smote_full.fit_resample(X_train_full_scaled, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62356ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best hyperparameters on balanced data\n",
    "best_svm = grid_search.best_estimator_  # pipeline with scaler + best SVM\n",
    "best_svm.fit(X_train_full_scaled, y_train_full)\n",
    "print(best_svm.named_steps['svm'])\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_svm, \"../data/storms_best_SVM.pkl\")\n",
    "print(\"Best model saved to ../data/storms_best_SVM.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b26606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = best_svm.predict(X_test_full_scaled)\n",
    "y_pred_proba = best_svm.predict_proba(X_test_full_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f0c88",
   "metadata": {},
   "source": [
    "# Results\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test_full, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_full, y_pred))\n",
    "\n",
    "if len(np.unique(y_test)) > 1:\n",
    "    f2_score = fbeta_score(y_test_full, y_pred, beta=2)\n",
    "    f1_score = fbeta_score(y_test_full, y_pred, beta=1)\n",
    "    recall = recall_score(y_test_full, y_pred)\n",
    "    avg_precision = average_precision_score(y_test_full, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_test_full, y_pred_proba)\n",
    "    \n",
    "    print(f\"Average F2 Score: {f2_score:.4f}\")\n",
    "    print(f\"Average F1 Score: {f1_score:.4f}\")\n",
    "    print(f\"Average Recall Score: {recall:.4f}\")\n",
    "    print(f\"Average Precision Score: {avg_precision:.4f}\")\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "plt.suptitle('Geomagnetic Storm Forecast Using Support Vector Machine', fontsize=16, fontweight='bold')\n",
    "\n",
    "class_names = ['No Storm', 'Storm']\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1, xticklabels=class_names, yticklabels=class_names)\n",
    "ax1.set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('True Label')\n",
    "ax1.set_xlabel('Predicted Label')\n",
    "\n",
    "# 2. Normalized Confusion Matrix\n",
    "cm_norm = pd.DataFrame(cm).apply(lambda x: x/sum(x), axis = 1)\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap='Blues', ax=ax2, xticklabels=class_names, yticklabels=class_names)\n",
    "ax2.set_title('Normalized Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('True Label')\n",
    "ax2.set_xlabel('Predicted Label')\n",
    "\n",
    "# 3. ROC Curve\n",
    "if len(np.unique(y_test_full)) > 1:\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    fpr, tpr, _ = roc_curve(y_test_full, y_pred_proba)\n",
    "    ax3.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "    ax3.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "    ax3.set_xlabel('False Positive Rate')\n",
    "    ax3.set_ylabel('True Positive Rate')\n",
    "    ax3.set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Precision-Recall Curve\n",
    "if len(np.unique(y_test_full)) > 1:\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    precision, recall, _ = precision_recall_curve(y_test_full, y_pred_proba)\n",
    "    ax4.plot(recall, precision, linewidth=2, label=f'PR (AP = {avg_precision:.3f})')\n",
    "    ax4.axhline(y=y_test_full.mean(), color='k', linestyle='--', linewidth=1, \n",
    "                label=f'Baseline ({y_test_full.mean():.3f})')\n",
    "    ax4.set_xlabel('Recall')\n",
    "    ax4.set_ylabel('Precision')\n",
    "    ax4.set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Prediction Probability Distribution\n",
    "ax6 = plt.subplot(2, 3, 5)\n",
    "ax6.hist(y_pred_proba[y_test_full == 0], bins=50, alpha=0.5, label='No Storm (True)', color='blue')\n",
    "ax6.hist(y_pred_proba[y_test_full == 1], bins=50, alpha=0.5, label='Storm (True)', color='red')\n",
    "ax6.set_xlabel('Predicted Probability')\n",
    "ax6.set_ylabel('Frequency')\n",
    "ax6.set_yscale('log')\n",
    "ax6.set_title('Distribution of Predicted Probabilities', fontsize=12, fontweight='bold')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/geomag_storm_SVM_results.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nVisualization saved as 'geomag_storm_SVM_results.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6be792",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba >= thresh).astype(int)\n",
    "    cm_thresh = confusion_matrix(y_test_full, y_pred_thresh)\n",
    "    tn, fp, fn, tp = cm_thresh.ravel() if cm_thresh.size == 4 else (cm_thresh[0,0], 0, 0, 0)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nThreshold = {thresh}\")\n",
    "    print(f\"  TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "    print(f\"  Precision: {precision:.3f}, Recall: {recall:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
